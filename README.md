# RAG-Pipeline-PanScience

ğŸ“„ DocTalk â€” Chat with Your Documents using RAG
DocTalk is an end-to-end Retrieval-Augmented Generation (RAG) application that enables users to upload .txt, .pdf, or .docx documents and interact with their content using natural language queries. It uses FAISS for semantic search, a HuggingFace embedding model for vectorization, Groq's LLaMA-3 for question answering, and MySQL for document persistence.

ğŸ§  Features
Upload up to 20 documents (.txt, .pdf, .docx)

Extracts, chunks, and embeds content into a FAISS vector store

Chat interface using Streamlit frontend

LLM-backed Q&A using Groq LLaMA-3

MySQL for structured document storage

HuggingFace-based embeddings for fast similarity search

ğŸ“ Project Structure
bash
Copy
Edit
RAG-Pipeline/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ backend_main.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ frontend_main.py
â”‚
â”œâ”€â”€ uploaded_documents/          # Automatically created at runtime
â”œâ”€â”€ vectorstore/                 # FAISS index stored here
â”œâ”€â”€ .env                         # API keys (Groq/OpenAI)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Setup Instructions
1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/yourusername/RAG-Pipeline.git
cd RAG-Pipeline
2. Environment Variables
Create a .env file in the root directory:

ini
Copy
Edit
GROQ_API_KEY=your_groq_api_key
OPENAI_API_KEY=your_openai_api_key  # (if used in future)
3. Install Dependencies
You can install dependencies either locally or using Docker.

âœ… Local Installation (Python 3.9+ recommended)
bash
Copy
Edit
pip install -r requirements.txt
ğŸ³ Docker (Optional)
bash
Copy
Edit
docker-compose up --build
Docker will set up the backend and the frontend; ensure your MySQL service is accessible to the container.

ğŸš€ Running the App
1. Start the FastAPI Backend
bash
Copy
Edit
uvicorn backend.backend_main:app --reload
Backend will be live at http://localhost:8000

2. Start the Streamlit Frontend
bash
Copy
Edit
cd frontend
streamlit run frontend_main.py
Frontend will be available at http://localhost:8501

ğŸ§ª API Endpoints
ğŸ”¹ /upload/ â€” Upload Documents
Method: POST

Accepts: Multipart files (max 20)

File Types: .txt, .pdf, .docx

Stores content in MySQL and vectorizes using FAISS

ğŸ”¹ /Ask/ â€” Ask a Question
Method: POST

Request Body: { "query": "Your question here" }

Returns: JSON response with the LLM-generated answer from the document context

ğŸ’¾ MySQL Integration
Make sure your MySQL server is running and accessible. You can update the connection string in:

backend/db/database.py

python
Copy
Edit
URL_DATABASE = 'mysql+pymysql://<user>:<password>@localhost:3306/RagApplication'
Example:
'mysql+pymysql://root:Vishal%40mysql123@localhost:3306/RagApplication'

ğŸ§  Tech Stack
Layer	Technology
Backend	FastAPI, SQLAlchemy, LangChain
Frontend	Streamlit
DB	MySQL
Embeddings	HuggingFace Transformers
Vector DB	FAISS
LLM	Groq - LLaMA-3
Chunking	LangChain Recursive Splitter

ğŸ›¡ï¸ Notes
Maximum of 1000 pages is supported per .pdf

Documents are chunked into 2000-character blocks with 1000-character overlap

Make sure FAISS index and MySQL DB are not deleted between sessions

ğŸ‘¨â€ğŸ’» Developer Tips
Clear previous FAISS index if needed by deleting vectorstore/faiss_index folder.

Ensure .env is not tracked by Git:

.gitignore

bash
Copy
Edit
.env
__pycache__/
vectorstore/
uploaded_documents/
ğŸ“· UI Preview
Upload your documents in the sidebar and start asking questions in the chat window.

ğŸ”§ Future Enhancements
Support for more file types (.xlsx, .pptx)

User authentication for document-specific chat

Integration with OpenAI or Anthropic models as fallback

Streamed chat interface

Deployment on cloud (e.g., Render, AWS)

ğŸ“œ License
This project is licensed under the MIT License.

ğŸ™Œ Acknowledgements
LangChain

HuggingFace

Groq

FAISS